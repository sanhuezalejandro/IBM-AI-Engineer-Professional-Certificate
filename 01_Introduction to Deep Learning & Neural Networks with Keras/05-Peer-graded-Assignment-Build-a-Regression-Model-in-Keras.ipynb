{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-graded Assignment: Build a Regression Model in Keras\n",
    "\n",
    "### A. Build a baseline model (5 marks) \n",
    "\n",
    "Use the Keras library to build a neural network with the following:\n",
    "\n",
    "- One hidden layer of 10 nodes, and a ReLU activation function\n",
    "\n",
    "- Use the adam optimizer and the mean squared error  as the loss function.\n",
    "\n",
    "1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the \n",
    "train_test_split\n",
    "helper function from Scikit-learn.\n",
    "\n",
    "2. Train the model on the training data using 50 epochs.\n",
    "\n",
    "3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n",
    "\n",
    "4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n",
    "\n",
    "5. Report the mean and the standard deviation of the mean squared errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import dataset\n",
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "concrete_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into predictors and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the number of predictors to *n_cols* since we will need this number when building our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = predictors.shape[1] # number of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 348us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 339us/step\n",
      "10/10 [==============================] - 0s 321us/step\n",
      "10/10 [==============================] - 0s 334us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 331us/step\n",
      "10/10 [==============================] - 0s 342us/step\n",
      "10/10 [==============================] - 0s 325us/step\n",
      "10/10 [==============================] - 0s 310us/step\n",
      "10/10 [==============================] - 0s 330us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 330us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 369us/step\n",
      "10/10 [==============================] - 0s 342us/step\n",
      "10/10 [==============================] - 0s 330us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 706us/step\n",
      "10/10 [==============================] - 0s 327us/step\n",
      "10/10 [==============================] - 0s 332us/step\n",
      "10/10 [==============================] - 0s 325us/step\n",
      "10/10 [==============================] - 0s 340us/step\n",
      "10/10 [==============================] - 0s 347us/step\n",
      "10/10 [==============================] - 0s 325us/step\n",
      "10/10 [==============================] - 0s 334us/step\n",
      "10/10 [==============================] - 0s 340us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 326us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 392us/step\n",
      "10/10 [==============================] - 0s 348us/step\n",
      "10/10 [==============================] - 0s 368us/step\n",
      "10/10 [==============================] - 0s 348us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 349us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 319us/step\n",
      "10/10 [==============================] - 0s 338us/step\n",
      "10/10 [==============================] - 0s 313us/step\n",
      "10/10 [==============================] - 0s 330us/step\n",
      "10/10 [==============================] - 0s 330us/step\n",
      "10/10 [==============================] - 0s 316us/step\n",
      "10/10 [==============================] - 0s 351us/step\n",
      "10/10 [==============================] - 0s 323us/step\n",
      "10/10 [==============================] - 0s 329us/step\n",
      "10/10 [==============================] - 0s 334us/step\n",
      "10/10 [==============================] - 0s 343us/step\n",
      "10/10 [==============================] - 0s 304us/step\n",
      "Mean Squared Error: 294.6865900859241\n",
      "Standard Deviation of MSE: 285.7902308414322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Step 1: Randomly split the data into training and test sets\n",
    "errors = []\n",
    "\n",
    "for _ in range(50):\n",
    "    predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.3)\n",
    "    # Convert Pandas DataFrames to NumPy arrays\n",
    "    predictors_train = predictors_train.values\n",
    "    predictors_test = predictors_test.values\n",
    "    target_train = target_train.values\n",
    "    target_test = target_test.values\n",
    "\n",
    "    # Step 2: Build the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(predictors_train.shape[1],)))\n",
    "    model.add(Dense(1))  # Output layer with 1 node for regression\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    model.fit(predictors_train, target_train, epochs=50, verbose=0)\n",
    "\n",
    "    # Step 4: Evaluate the model on the test data\n",
    "    predictions = model.predict(predictors_test)\n",
    "    mse = mean_squared_error(target_test, predictions)\n",
    "    errors.append(mse)\n",
    "\n",
    "# Step 5: Report the mean and standard deviation of the mean squared errors\n",
    "mean_mse = np.mean(errors)\n",
    "std_mse = np.std(errors)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_mse}\")\n",
    "print(f\"Standard Deviation of MSE: {std_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Normalize the data (5 marks) \n",
    "\n",
    "Repeat Part A but use a normalized version of the data. Recall that one way to normalize the data is by subtracting the mean from the individual predictors and dividing by the standard deviation.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 371us/step\n",
      "10/10 [==============================] - 0s 358us/step\n",
      "10/10 [==============================] - 0s 387us/step\n",
      "10/10 [==============================] - 0s 339us/step\n",
      "10/10 [==============================] - 0s 346us/step\n",
      "10/10 [==============================] - 0s 350us/step\n",
      "10/10 [==============================] - 0s 372us/step\n",
      "10/10 [==============================] - 0s 333us/step\n",
      "10/10 [==============================] - 0s 383us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 339us/step\n",
      "10/10 [==============================] - 0s 338us/step\n",
      "10/10 [==============================] - 0s 381us/step\n",
      "10/10 [==============================] - 0s 343us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 382us/step\n",
      "10/10 [==============================] - 0s 333us/step\n",
      "10/10 [==============================] - 0s 331us/step\n",
      "10/10 [==============================] - 0s 391us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 347us/step\n",
      "10/10 [==============================] - 0s 358us/step\n",
      "10/10 [==============================] - 0s 371us/step\n",
      "10/10 [==============================] - 0s 374us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 319us/step\n",
      "10/10 [==============================] - 0s 341us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 418us/step\n",
      "10/10 [==============================] - 0s 340us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 365us/step\n",
      "10/10 [==============================] - 0s 338us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 316us/step\n",
      "10/10 [==============================] - 0s 329us/step\n",
      "10/10 [==============================] - 0s 339us/step\n",
      "10/10 [==============================] - 0s 372us/step\n",
      "10/10 [==============================] - 0s 318us/step\n",
      "10/10 [==============================] - 0s 361us/step\n",
      "10/10 [==============================] - 0s 336us/step\n",
      "10/10 [==============================] - 0s 391us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 321us/step\n",
      "10/10 [==============================] - 0s 354us/step\n",
      "10/10 [==============================] - 0s 320us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 385us/step\n",
      "10/10 [==============================] - 0s 348us/step\n",
      "Mean Squared Error (Normalized): 347.5443665902096\n",
      "Standard Deviation of MSE (Normalized): 81.59316526961535\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Assuming 'concrete_data' is your DataFrame containing the data\n",
    "\n",
    "# Step 1: Randomly split the data into training and test sets\n",
    "errors_normalized = []\n",
    "\n",
    "for _ in range(50):\n",
    "    predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.3)\n",
    "\n",
    "    # Normalize the predictors using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    predictors_train_scaled = scaler.fit_transform(predictors_train)\n",
    "    predictors_test_scaled = scaler.transform(predictors_test)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    target_train_np = target_train.values\n",
    "    target_test_np = target_test.values\n",
    "\n",
    "    # Step 2: Build the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(predictors_train_scaled.shape[1],)))\n",
    "    model.add(Dense(1))  # Output layer with 1 node for regression\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    model.fit(predictors_train_scaled, target_train_np, epochs=50, verbose=0)\n",
    "\n",
    "    # Step 4: Evaluate the model on the test data\n",
    "    predictions = model.predict(predictors_test_scaled)\n",
    "    mse = mean_squared_error(target_test_np, predictions)\n",
    "    errors_normalized.append(mse)\n",
    "\n",
    "# Step 5: Report the mean and standard deviation of the mean squared errors for normalized data\n",
    "mean_mse_normalized = np.mean(errors_normalized)\n",
    "std_mse_normalized = np.std(errors_normalized)\n",
    "\n",
    "print(f\"Mean Squared Error (Normalized): {mean_mse_normalized}\")\n",
    "print(f\"Standard Deviation of MSE (Normalized): {std_mse_normalized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like normalizing the data had an impact on the mean squared error. The mean squared error increased from 294.69 (in the original, non-normalized case) to 347.54 in the normalized case. The standard deviation of the mean squared error also decreased from 285.79 to 81.59.\n",
    "\n",
    "In general, normalizing data is a common practice in machine learning to ensure that features are on a similar scale, which can improve the training stability and convergence of the neural network. However, the effect on the performance (in terms of mean squared error) can vary depending on the specific characteristics of the data and the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Increate the number of epochs (5 marks)\n",
    "\n",
    "Repeat Part B but use 100 epochs this time for training.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 362us/step\n",
      "10/10 [==============================] - 0s 329us/step\n",
      "10/10 [==============================] - 0s 347us/step\n",
      "10/10 [==============================] - 0s 346us/step\n",
      "10/10 [==============================] - 0s 361us/step\n",
      "10/10 [==============================] - 0s 346us/step\n",
      "10/10 [==============================] - 0s 342us/step\n",
      "10/10 [==============================] - 0s 326us/step\n",
      "10/10 [==============================] - 0s 386us/step\n",
      "10/10 [==============================] - 0s 324us/step\n",
      "10/10 [==============================] - 0s 320us/step\n",
      "10/10 [==============================] - 0s 357us/step\n",
      "10/10 [==============================] - 0s 385us/step\n",
      "10/10 [==============================] - 0s 336us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 351us/step\n",
      "10/10 [==============================] - 0s 350us/step\n",
      "10/10 [==============================] - 0s 326us/step\n",
      "10/10 [==============================] - 0s 340us/step\n",
      "10/10 [==============================] - 0s 347us/step\n",
      "10/10 [==============================] - 0s 370us/step\n",
      "10/10 [==============================] - 0s 360us/step\n",
      "10/10 [==============================] - 0s 326us/step\n",
      "10/10 [==============================] - 0s 331us/step\n",
      "10/10 [==============================] - 0s 352us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 414us/step\n",
      "10/10 [==============================] - 0s 311us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 332us/step\n",
      "10/10 [==============================] - 0s 333us/step\n",
      "10/10 [==============================] - 0s 318us/step\n",
      "10/10 [==============================] - 0s 334us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 366us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 385us/step\n",
      "10/10 [==============================] - 0s 349us/step\n",
      "10/10 [==============================] - 0s 338us/step\n",
      "10/10 [==============================] - 0s 319us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 309us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 368us/step\n",
      "10/10 [==============================] - 0s 342us/step\n",
      "10/10 [==============================] - 0s 344us/step\n",
      "10/10 [==============================] - 0s 330us/step\n",
      "10/10 [==============================] - 0s 381us/step\n",
      "10/10 [==============================] - 0s 329us/step\n",
      "10/10 [==============================] - 0s 355us/step\n",
      "Mean Squared Error (Normalized): 166.83116811074854\n",
      "Standard Deviation of MSE (Normalized): 16.580480432717486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Assuming 'concrete_data' is your DataFrame containing the data\n",
    "\n",
    "# Step 1: Randomly split the data into training and test sets\n",
    "errors_normalized = []\n",
    "\n",
    "for _ in range(50):\n",
    "    predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.3)\n",
    "\n",
    "    # Normalize the predictors using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    predictors_train_scaled = scaler.fit_transform(predictors_train)\n",
    "    predictors_test_scaled = scaler.transform(predictors_test)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    target_train_np = target_train.values\n",
    "    target_test_np = target_test.values\n",
    "\n",
    "    # Step 2: Build the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(predictors_train_scaled.shape[1],)))\n",
    "    model.add(Dense(1))  # Output layer with 1 node for regression\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    model.fit(predictors_train_scaled, target_train_np, epochs=100, verbose=0)\n",
    "\n",
    "    # Step 4: Evaluate the model on the test data\n",
    "    predictions = model.predict(predictors_test_scaled)\n",
    "    mse = mean_squared_error(target_test_np, predictions)\n",
    "    errors_normalized.append(mse)\n",
    "\n",
    "# Step 5: Report the mean and standard deviation of the mean squared errors for normalized data\n",
    "mean_mse_normalized = np.mean(errors_normalized)\n",
    "std_mse_normalized = np.std(errors_normalized)\n",
    "\n",
    "print(f\"Mean Squared Error (Normalized): {mean_mse_normalized}\")\n",
    "print(f\"Standard Deviation of MSE (Normalized): {std_mse_normalized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that increasing the number of epochs from 50 to 100 had a noticeable effect on the mean squared error. The mean squared error decreased substantially from 347.54 (with 50 epochs) to 166.83 (with 100 epochs), suggesting that the model may benefit from more training epochs.\n",
    "\n",
    "This behavior is not uncommon. Increasing the number of epochs allows the model more opportunities to adjust its weights and learn from the data. However, it's essential to monitor for signs of overfitting, where the model may start fitting the training data too closely and not generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Increase the number of hidden layers (5 marks)\n",
    "\n",
    "Repeat part B but use a neural network with the following instead:\n",
    "\n",
    "- Three hidden layers, each of 10 nodes and ReLU activation function.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 341us/step\n",
      "10/10 [==============================] - 0s 347us/step\n",
      "10/10 [==============================] - 0s 341us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 347us/step\n",
      "10/10 [==============================] - 0s 366us/step\n",
      "10/10 [==============================] - 0s 380us/step\n",
      "10/10 [==============================] - 0s 348us/step\n",
      "10/10 [==============================] - 0s 352us/step\n",
      "10/10 [==============================] - 0s 344us/step\n",
      "10/10 [==============================] - 0s 353us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 347us/step\n",
      "10/10 [==============================] - 0s 331us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 330us/step\n",
      "10/10 [==============================] - 0s 358us/step\n",
      "10/10 [==============================] - 0s 341us/step\n",
      "10/10 [==============================] - 0s 358us/step\n",
      "10/10 [==============================] - 0s 323us/step\n",
      "10/10 [==============================] - 0s 340us/step\n",
      "10/10 [==============================] - 0s 340us/step\n",
      "10/10 [==============================] - 0s 348us/step\n",
      "10/10 [==============================] - 0s 350us/step\n",
      "10/10 [==============================] - 0s 343us/step\n",
      "10/10 [==============================] - 0s 386us/step\n",
      "10/10 [==============================] - 0s 346us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 334us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 332us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 474us/step\n",
      "10/10 [==============================] - 0s 362us/step\n",
      "10/10 [==============================] - 0s 335us/step\n",
      "10/10 [==============================] - 0s 317us/step\n",
      "10/10 [==============================] - 0s 344us/step\n",
      "10/10 [==============================] - 0s 375us/step\n",
      "10/10 [==============================] - 0s 337us/step\n",
      "10/10 [==============================] - 0s 328us/step\n",
      "10/10 [==============================] - 0s 336us/step\n",
      "10/10 [==============================] - 0s 348us/step\n",
      "10/10 [==============================] - 0s 356us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 340us/step\n",
      "10/10 [==============================] - 0s 375us/step\n",
      "10/10 [==============================] - 0s 345us/step\n",
      "10/10 [==============================] - 0s 332us/step\n",
      "Mean Squared Error (Three Hidden Layers): 126.68835618480627\n",
      "Standard Deviation of MSE (Three Hidden Layers): 18.220522734685826\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Assuming 'concrete_data' is your DataFrame containing the data\n",
    "\n",
    "# Step 1: Randomly split the data into training and test sets\n",
    "errors_three_layers = []\n",
    "\n",
    "for _ in range(50):\n",
    "    predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.3)\n",
    "\n",
    "    # Normalize the predictors using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    predictors_train_scaled = scaler.fit_transform(predictors_train)\n",
    "    predictors_test_scaled = scaler.transform(predictors_test)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    target_train_np = target_train.values\n",
    "    target_test_np = target_test.values\n",
    "\n",
    "    # Step 2: Build the neural network with three hidden layers\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(predictors_train_scaled.shape[1],)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer with 1 node for regression\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    model.fit(predictors_train_scaled, target_train_np, epochs=50, verbose=0)\n",
    "\n",
    "    # Step 4: Evaluate the model on the test data\n",
    "    predictions = model.predict(predictors_test_scaled)\n",
    "    mse = mean_squared_error(target_test_np, predictions)\n",
    "    errors_three_layers.append(mse)\n",
    "\n",
    "# Step 5: Report the mean and standard deviation of the mean squared errors for three hidden layers\n",
    "mean_mse_three_layers = np.mean(errors_three_layers)\n",
    "std_mse_three_layers = np.std(errors_three_layers)\n",
    "\n",
    "print(f\"Mean Squared Error (Three Hidden Layers): {mean_mse_three_layers}\")\n",
    "print(f\"Standard Deviation of MSE (Three Hidden Layers): {std_mse_three_layers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error with three hidden layers (126.69) is significantly lower than the mean squared error from Step B with normalization (347.54). This suggests that the model with three hidden layers performed better in terms of prediction accuracy compared to the model with a single hidden layer.\n",
    "\n",
    "In comparison with Step C, where the mean squared error was 166.83 (with 100 epochs and normalization), the mean squared error further decreased to 126.69 when three hidden layers with 10 nodes each and ReLU activation function were introduced.\n",
    "\n",
    "It indicates that the increased model complexity introduced by adding more hidden layers and nodes allowed the neural network to capture more intricate patterns in the data, leading to improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
